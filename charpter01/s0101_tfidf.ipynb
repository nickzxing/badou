{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 案例1：基于TF-IDF的关键词提取\n",
    "## 开发步骤如下：\n",
    "> * Step 1：原始数据预处理\n",
    "> * Step 2：产生IDF词表\n",
    "> * Step 3：提取句子的关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "  \n",
    "file_path_dir = './d0101/data'\n",
    "raw_path = './d0101/raw.data'\n",
    "idf_path = './d0101/idf.data'\n",
    "\n",
    "def read_file_handler(f):\n",
    "    fd = open(f, 'r', encoding='utf-8')\n",
    "    return fd\n",
    "\n",
    "file_raw_out = open(raw_path, 'w', encoding='utf-8')\n",
    "\n",
    "# 遍历整个原始数据目录，将零散的文章整合到一个文件中，便于后续数据处理\n",
    "file_name = 0\n",
    "for fd in os.listdir(file_path_dir):\n",
    "    file_path = file_path_dir + '/' + fd\n",
    "\n",
    "    content_list = []\n",
    "\n",
    "    file_fd = read_file_handler(file_path)\n",
    "    for line in file_fd:\n",
    "        content_list.append(line.strip())\n",
    "\n",
    "    content = '\\t'.join([str(file_name), ' '.join(content_list)]) + '\\n'\n",
    "    file_raw_out.writelines(content)\n",
    "    \n",
    "    file_name += 1\n",
    "\n",
    "file_raw_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 2\n",
    "docs_cnt = file_name\n",
    "wc_tulist = []\n",
    "\n",
    "with open(raw_path, 'r', encoding='utf-8') as fd:\n",
    "    for line in fd:\n",
    "        # 遍历每一篇文章，文章=line\n",
    "        ss = line.strip().split('\\t')\n",
    "        if len(ss) != 2:\n",
    "            continue\n",
    "        # 对文章的解析，区分出文章的名字和文章的内容   \n",
    "        file_name, file_content = ss\n",
    "        # 对文章的内容进行切词，因为内容已经按“ ”空格区分好了，所以直接按空格做split就好\n",
    "        word_list = file_content.strip().split(' ')\n",
    "        \n",
    "        # 去重：对于idf，只关心词有没有出现在文章中，至于出现多少次，并不关心\n",
    "        word_set = set(word_list)\n",
    "\n",
    "        for word in word_set:\n",
    "            # 对于每个关键词，打一个标记“1”，来标识该次出现过\n",
    "            wc_tulist.append((word, '1'))\n",
    "\n",
    "# 将内容输出到指定目标文件中去\n",
    "file_idf_out = open(idf_path, 'w', encoding='utf-8')\n",
    "\n",
    "# 按照词的字典序，进行排序\n",
    "wc_sort_tulist = sorted(wc_tulist, key=lambda x: x[0])\n",
    "\n",
    "current_word = None\n",
    "sum = 0\n",
    "for tu in wc_sort_tulist:  \n",
    "    word, val = tu\n",
    "\n",
    "    if current_word == None:\n",
    "        current_word = word\n",
    "\n",
    "    if current_word != word:\n",
    "        # 通过idf计算公式，得到每个关键词的idf score\n",
    "        idf = math.log(float(docs_cnt) / (float(sum) + 1.0))\n",
    "        content = '\\t'.join([current_word, str(idf)]) + '\\n'\n",
    "        file_idf_out.write(content)\n",
    "        current_word = word\n",
    "        sum = 0\n",
    "\n",
    "    sum += int(val)\n",
    "\n",
    "idf = math.log(float(docs_cnt) / (float(sum) + 1.0))\n",
    "content = '\\t'.join([current_word, str(idf)]) + '\\n'\n",
    "file_idf_out.write(content)\n",
    "\n",
    "file_idf_out.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我们 1.0750817912579156\n",
      "带来 1.4518916436057039\n",
      "阿里巴巴 2.8552804380848946\n",
      "希望 1.9321168269229776\n",
      "差 2.1795250002368185\n"
     ]
    }
   ],
   "source": [
    "# Step 3\n",
    "input_str = '我们 带来 阿里巴巴 希望 差'\n",
    "\n",
    "token_idf_dict = {}\n",
    "# 将idf字典加载到内存\n",
    "with open(idf_path, 'r', encoding='utf-8') as fd:\n",
    "    for line in fd:\n",
    "        ss = line.strip().split('\\t')\n",
    "        if len(ss) != 2:\n",
    "            continue\n",
    "        token, idf_score = ss\n",
    "        token_idf_dict[token] = idf_score\n",
    "\n",
    "def get_tfidf(input_str):\n",
    "    token_dict = {}\n",
    "    # 对输入字符串的每一个词，计算tf\n",
    "    for t in input_str.strip().split(' '):\n",
    "        if t not in token_dict:\n",
    "            token_dict[t] = 1\n",
    "        else:\n",
    "            token_dict[t] += 1\n",
    "\n",
    "    # res_tu_list = []\n",
    "    for k, v in token_dict.items():\n",
    "        tf_score = token_dict[k]\n",
    "        if k not in token_idf_dict:\n",
    "            continue\n",
    "        idf_score = token_idf_dict[k]\n",
    "        tf_idf = tf_score * float(idf_score)\n",
    "        yield (k, tf_idf)\n",
    "\n",
    "for k, v in get_tfidf(input_str):\n",
    "    print(k, v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 案例2：相似度公式\n",
    "## 开发步骤如下：\n",
    "> * Step 1：Cosine\n",
    "> * Step 2：Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Cosine\n",
    "\n",
    "input1_str = '我们 带来 阿里巴巴 希望 差 差 差'\n",
    "# input2_str = '我们 带来 阿里巴巴 好 好 好'\n",
    "# input2_str = '我们 带来 搜狐 好 好 好'\n",
    "input2_str = '我们 带来 阿里巴巴 希望 差 差 差'\n",
    "\n",
    "def cosine(input1_str, input2_str):\n",
    "    t1_dict = {}\n",
    "    sum = 0.\n",
    "    for k, v in get_tfidf(input1_str):\n",
    "        sum += pow(v, 2)\n",
    "    sum = math.sqrt(sum)\n",
    "    for k, v in get_tfidf(input1_str):\n",
    "        t1_dict[k] = float(v / sum)\n",
    "\n",
    "    sum = 0.\n",
    "    for k, v in get_tfidf(input2_str):\n",
    "        sum += pow(v, 2)\n",
    "    sum = math.sqrt(sum)\n",
    "    \n",
    "    final_score = 0.\n",
    "    for k, v in get_tfidf(input2_str):\n",
    "        if k not in t1_dict:\n",
    "            continue\n",
    "        s1 = t1_dict[k]\n",
    "        s2 = float(v / sum)\n",
    "        \n",
    "        final_score += s1 * s2\n",
    "    return final_score\n",
    "        \n",
    "print(cosine(input1_str, input2_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Jaccard\n",
    "input1_str = '我们 带来 阿里巴巴 希望 差 差 差'\n",
    "# input2_str = '我们 带来 阿里巴巴 好 好 好'\n",
    "input2_str = '我们 带来'\n",
    "\n",
    "def jaccard(input1_str, input2_str):\n",
    "    s1_set = set(input1_str.strip().split(' '))\n",
    "    s2_set = set(input2_str.strip().split(' '))\n",
    "    \n",
    "    score = 0.\n",
    "    s1_s2_join = s1_set & s2_set\n",
    "    len1 = len(s1_s2_join)\n",
    "    \n",
    "    s1_s2_union = s1_set | s2_set\n",
    "    len2 = len(s1_s2_union)\n",
    "    \n",
    "    return float(len1) / float(len2)\n",
    "    \n",
    "print(jaccard(input1_str, input2_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "badou",
   "language": "python",
   "display_name": "PyCharm(badou)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}